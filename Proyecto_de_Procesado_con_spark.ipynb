{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Procesado_spark",
      "provenance": [],
      "collapsed_sections": [
        "Y4a_8p48g16-",
        "56YolyX4BzCl"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10I3ZDsb3r72"
      },
      "source": [
        "\n",
        "<CENTER><img src=https://upload.wikimedia.org/wikipedia/commons/8/84/URJC_logo.svg></CENTER>\n",
        "\n",
        "\n",
        " \n",
        "## <center>  ANÁLISIS DEL ESTADO ANÍMICO DE LA SOCIEDAD EN EL COVID-19 A TRAVÉS DE TWEETS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXQ1qPur37NX"
      },
      "source": [
        "## Importamos los datos y las librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI8K5x0wkIKS"
      },
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from os import listdir, mkdir\n",
        "\n",
        "import re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41EhB_oLrCpb",
        "outputId": "e5b820ad-20f6-4a54-bf5c-17175b4afdc5"
      },
      "source": [
        "!pip install pyspark[sql]\n",
        "\n",
        "from __future__ import print_function\n",
        "from functools import wraps\n",
        "import pyspark as spark\n",
        "from pyspark import SparkConf\n",
        "import time\n",
        "from operator import add\n",
        "import os \n",
        "from subprocess import STDOUT, check_call, check_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark[sql] in /usr/local/lib/python3.7/dist-packages (3.1.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark[sql]) (0.10.9)\n",
            "Requirement already satisfied: pyarrow>=1.0.0; extra == \"sql\" in /usr/local/lib/python3.7/dist-packages (from pyspark[sql]) (3.0.0)\n",
            "Requirement already satisfied: pandas>=0.23.2; extra == \"sql\" in /usr/local/lib/python3.7/dist-packages (from pyspark[sql]) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow>=1.0.0; extra == \"sql\"->pyspark[sql]) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2; extra == \"sql\"->pyspark[sql]) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2; extra == \"sql\"->pyspark[sql]) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.2; extra == \"sql\"->pyspark[sql]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GKlc-75rFxf",
        "outputId": "61b38e4d-c7b2-4f64-940b-d6538113344f"
      },
      "source": [
        "def setup_sra_tool(url):\n",
        "  !wget $url\n",
        "  !gunzip sratoolkit.2.9.6-1-ubuntu64.tar.gz\n",
        "  !tar -xf sratoolkit.2.9.6-1-ubuntu64.tar\n",
        "  \n",
        "#download SRA file and extract fastq\n",
        "def get_sra(url, sra_path):\n",
        "  os.chdir('/content')\n",
        "  !wget $url\n",
        "  sra_name = url[-11:]\n",
        "  os.chdir(sra_path)\n",
        "  !./fastq-dump /content/$sra_name -O /content/\n",
        "  os.chdir('/content')\n",
        "  \n",
        "# url of SRA tool kit \n",
        "url_tk= 'https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.9.6-1/sratoolkit.2.9.6-1-ubuntu64.tar.gz'\n",
        "setup_sra_tool(url_tk)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-02 09:18:38--  https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.9.6-1/sratoolkit.2.9.6-1-ubuntu64.tar.gz\n",
            "Resolving ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)... 165.112.9.228, 165.112.9.229, 2607:f220:41e:250::12, ...\n",
            "Connecting to ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)|165.112.9.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84492294 (81M) [application/x-gzip]\n",
            "Saving to: ‘sratoolkit.2.9.6-1-ubuntu64.tar.gz.1’\n",
            "\n",
            " sratoolkit.2.9.6-1  61%[===========>        ]  49.38M  24.8MB/s               ^C\n",
            "gzip: sratoolkit.2.9.6-1-ubuntu64.tar already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZs4OPQ5rLKf"
      },
      "source": [
        "def set_conf():\n",
        "    conf = SparkConf().setAppName(\"App\")\n",
        "    conf = (conf.setMaster('local[*]')\n",
        "      .set('spark.executor.memory', '4G')\n",
        "      .set('spark.driver.memory', '16G')\n",
        "      .set('spark.driver.maxResultSize', '8G'))\n",
        "    return conf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAa7m0strLtK"
      },
      "source": [
        "sc = spark.SparkContext.getOrCreate(conf=set_conf())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqW6rXYivEEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3f5684-32ff-4d6c-b4e3-3fbe6bb8a7d6"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyMm8d6wvEGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebaeefc8-dbf0-4f80-c4ec-6cfa5c5aad2d"
      },
      "source": [
        "dir = \"/content/drive/MyDrive/MI_PMD/\"\n",
        "folders = os.listdir(dir)\n",
        "print(folders)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Corona_NLP_test.csv', '.ipynb_checkpoints', 'Corona_NLP_train.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7LH_Nvu5PXY"
      },
      "source": [
        "## Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hR0p9z7YCpN"
      },
      "source": [
        "df_train = pd.read_csv(dir+\"Corona_NLP_train.csv\",encoding = \"IBM437\")\n",
        "df_test = pd.read_csv(dir+\"Corona_NLP_test.csv\",encoding = \"IBM437\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peYCk4ZyqnMx"
      },
      "source": [
        "#df_train=sc.textFile(dir+\"Corona_NLP_train.csv\")\n",
        "#df_test=sc.textFile(dir+\"Corona_NLP_test.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtggUEVevE2g"
      },
      "source": [
        "df_train = pd.read_csv(\"Corona_NLP_train.csv\",encoding = \"IBM437\")\n",
        "df_test = pd.read_csv(\"Corona_NLP_test.csv\",encoding = \"IBM437\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr6kg9nm92Ib"
      },
      "source": [
        "https://docs.python.org/3/library/codecs.html#standard-encodings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwRtVWKGvERc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "0a6c26d8-0fe5-479a-fc41-894c0688edee"
      },
      "source": [
        "#Mostramos gráficamente la distribución de los datos\n",
        "fig, ax = plt.subplots(1,2,figsize=(15,7))\n",
        "ax[0].hist(df_train[\"Sentiment\"],alpha = 0.5, color = 'r')\n",
        "ax[1].hist(df_test[\"Sentiment\"],alpha = 0.5, color = 'g')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IodZBRdKHJc"
      },
      "source": [
        "Analizamos la cantidad de valores nulos y a qué columnas afectan. Se puede ver que afecta únicamente a localización por lo que no tendría sentido eliminar aquellos usuarios de los que no tenemos localización cuando sí se puede realizar el objetivo principal del proyecto consistente en relacionar el tweet con los sentimientos expresados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP-fOgBx4DfB",
        "outputId": "a2582212-8eeb-425f-90ea-1d319d741f4f"
      },
      "source": [
        "df_train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UserName            0\n",
              "ScreenName          0\n",
              "Location         8590\n",
              "TweetAt             0\n",
              "OriginalTweet       0\n",
              "Sentiment           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2c8yDpw4Dhc",
        "outputId": "14be7af5-e44d-47c1-e238-41813cb0df3e"
      },
      "source": [
        "df_test.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UserName           0\n",
              "ScreenName         0\n",
              "Location         834\n",
              "TweetAt            0\n",
              "OriginalTweet      0\n",
              "Sentiment          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0CSm2DqLLhR"
      },
      "source": [
        "### Preprocesado de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Y4FLsB5FhJ"
      },
      "source": [
        "def clean_text_round1(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?¿\\]\\%', ' ', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "def clean_text_round2(text):\n",
        "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
        "    text = re.sub('[‘’“”…«»]', '', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    return text\n",
        "def eliminar_tilde(text):\n",
        "    text=text.replace(\"á\",\"a\")\n",
        "    text=text.replace(\"é\",\"e\")\n",
        "    text=text.replace(\"í\",\"i\")\n",
        "    text=text.replace(\"ó\",\"o\")\n",
        "    text=text.replace(\"ú\",\"u\")\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-pe036w5Fjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7532a162-981d-414a-9a59-98d492d8e62f"
      },
      "source": [
        "for k in range(0,len(df_train['OriginalTweet'])):\n",
        "    clean1=clean_text_round1(df_train['OriginalTweet'][k])\n",
        "    clean2=clean_text_round2(clean1)\n",
        "    df_train['OriginalTweet'][k]=eliminar_tilde(clean2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAd1mTqI0qNG",
        "outputId": "d9ed59ef-ebde-4dfb-c94d-82c14aea7688"
      },
      "source": [
        "for k in range(0,len(df_test['OriginalTweet'])):\n",
        "    clean1=clean_text_round1(df_test['OriginalTweet'][k])\n",
        "    clean2=clean_text_round2(clean1)\n",
        "    df_test['OriginalTweet'][k]=eliminar_tilde(clean2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-hHfDWm5Fl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7842320c-0cf4-4b99-a3c4-6487baaf0618"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "print(\"\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "texto = df_train.OriginalTweet.str.cat(sep=' ')\n",
        "\n",
        "#function to split text into word\n",
        "tokens = word_tokenize(texto)\n",
        "\n",
        "vocabulary = set(tokens)\n",
        "print(\"Cantidad de palabras distintas: \", len(vocabulary))\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"Ejemplos de stopwords:\", list(stop_words)[0:10])\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "frequency_dist = nltk.FreqDist(tokens)\n",
        "print(\"Diez palabras con mayor frecuencia: \", sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "\n",
            "Cantidad de palabras distintas:  57451\n",
            "Ejemplos de stopwords: ['being', 'i', 'own', \"you've\", 'under', 'so', \"weren't\", 'shouldn', \"shan't\", 'but']\n",
            "Diez palabras con mayor frecuencia:  ['co', 'https', 'coronavirus', 'covid', 'prices', 'food', 'supermarket', 'store', 'grocery', 'people']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is6rcXyC2IQ3",
        "outputId": "3d79e5cb-8228-41bc-ac8f-b2dd409f034a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "print(\"\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "texto = df_test.OriginalTweet.str.cat(sep=' ')\n",
        "\n",
        "#function to split text into word\n",
        "tokens = word_tokenize(texto)\n",
        "\n",
        "vocabulary = set(tokens)\n",
        "print(\"Cantidad de palabras distintas: \", len(vocabulary))\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"Ejemplos de stopwords:\", list(stop_words)[0:10])\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "frequency_dist = nltk.FreqDist(tokens)\n",
        "print(\"Diez palabras con mayor frecuencia: \", sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "Cantidad de palabras distintas:  11971\n",
            "Ejemplos de stopwords: ['being', 'i', 'own', \"you've\", 'under', 'so', \"weren't\", 'shouldn', \"shan't\", 'but']\n",
            "Diez palabras con mayor frecuencia:  ['covid', 'co', 'https', 'coronavirus', 'food', 'store', 'grocery', 'stock', 'people', 'amp']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoeMfJXb09-4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_uRhs__0-nD"
      },
      "source": [
        "X_train=df_train[\"OriginalTweet\"]\n",
        "y_train=df_train[\"Sentiment\"]\n",
        "X_test=df_test[\"OriginalTweet\"]\n",
        "y_test=df_test[\"Sentiment\"]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "train_vectors = vectorizer.fit_transform(X_train)\n",
        "test_vectors = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95OXonxUklgT"
      },
      "source": [
        "dicccionario={\"y\":y_train,\"x\":train_vectors}\n",
        "diccionario2={\"y\":y_test,\"x\":test_vectors}\n",
        "a=pd.DataFrame.from_dict(dicccionario)\n",
        "b=pd.DataFrame.from_dict(diccionario2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP_HifqOmV0P",
        "outputId": "f4c0d0b4-1cd2-42cb-d660-fa5db8b1b28b"
      },
      "source": [
        "type(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKT9KrsaxPAt"
      },
      "source": [
        "X_df_train=a.to_csv(index=False, header=False)\n",
        "X_df_test=b.to_csv(index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRKGucbUzwD5"
      },
      "source": [
        "X_train=sc.textFile(X_df_train)\n",
        "x_test=sc.textFile(X_df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjcEbKMmf297",
        "outputId": "248a7776-6f81-40c0-9418-b330031d639a"
      },
      "source": [
        "type(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQUIIhpp-jiI"
      },
      "source": [
        "# Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpY1e4AtnFeB",
        "outputId": "7662c438-8a46-4a2a-b76d-824bc6944703"
      },
      "source": [
        "pip install findspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting findspark\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/2d/2e39f9a023479ea798eed4351cd66f163ce61e00c717e03c37109f00c0f2/findspark-1.4.2-py2.py3-none-any.whl\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ClIBUQsm52D"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "# Create a spark session\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "YGiyxJnxpiG6",
        "outputId": "e0d2d58e-2b29-46d5-e697-a70d2612dd4b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Enable Arrow-based columnar data transfers\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "\n",
        "# Generate a pandas DataFrame\n",
        "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
        "\n",
        "# Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
        "result_pdf = df.select(\"*\").toPandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-f08e15aca7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Enable Arrow-based columnar data transfers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.execution.arrow.enabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Generate a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.conf' has no attribute 'set'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "NcUdVtnsfROS",
        "outputId": "556df7c6-b0c5-4ccf-e7d8-2b4942a299ba"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "sqlContext.createDataFrame(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'numpy.ndarray'>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m             \u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"not supported type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: not supported type: <class 'numpy.ndarray'>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unable to infer the type of the field {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unable to infer the type of the field data.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m             \u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"not supported type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: not supported type: <class 'scipy.sparse.csr.csr_matrix'>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-ab70d36c9fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \"\"\"\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregisterDataFrameAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m--> 674\u001b[0;31m                 data, schema, samplingRatio, verifySchema)\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unable to infer the type of the field {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unable to infer the type of the field x."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "s9lYmUF3ewNU",
        "outputId": "1c1c0995-815e-49f0-8a4a-c7a5ee52244c"
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Load training data\n",
        "data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_multiclass_classification_data.txt\")\n",
        "\n",
        "# Split the data into train and test\n",
        "splits = data.randomSplit([0.6, 0.4], 1234)\n",
        "train = splits[0]\n",
        "test = splits[1]\n",
        "\n",
        "# specify layers for the neural network:\n",
        "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
        "# and output of size 3 (classes)\n",
        "layers = [4, 5, 4, 3]\n",
        "\n",
        "# create the trainer and set its parameters\n",
        "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
        "\n",
        "# train the model\n",
        "model = trainer.fit(train)\n",
        "\n",
        "# compute accuracy on the test set\n",
        "result = model.transform(test)\n",
        "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-9af4e56c4b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libsvm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/mllib/sample_multiclass_classification_data.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Split the data into train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark' has no attribute 'read'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "yELAeJLqw4AD",
        "outputId": "2bf89f94-c8c2-439f-860d-d12bd6ce0d85"
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "layers = [4, 5, 4, 3]\n",
        "\n",
        "# create the trainer and set its parameters\n",
        "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
        "\n",
        "# train the model\n",
        "model = trainer.fit(X_train)\n",
        "\n",
        "# compute accuracy on the test set\n",
        "result = model.transform(x_test)\n",
        "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-cdf6fa05d56d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# compute accuracy on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute '_jdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVuZ4cf9w4xo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5ec3764-c225-43f3-8e4a-e782b81a15c1"
      },
      "source": [
        "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
        "from pyspark.mllib.util import MLUtils\n",
        "\n",
        "\n",
        "\n",
        "# Load and parse the data file.\n",
        "#data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
        "\n",
        "# Split data approximately into training (60%) and test (40%)\n",
        "training, test = X_train.randomSplit([0.6, 0.4])\n",
        "\n",
        "# Train a naive Bayes model.\n",
        "model = NaiveBayes.train(training, 1.0)\n",
        "\n",
        "# Make prediction and test accuracy.\n",
        "predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))\n",
        "accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
        "print('model accuracy {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-8b0f006c8951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train a naive Bayes model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Make prediction and test accuracy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, data, lambda_)\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \"\"\"\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`data` should be an RDD of LabeledPoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \"\"\"\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \"\"\"\n\u001b[1;32m   1532\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1533\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1534\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2935\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o89.partitions.\n: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI:  10896)\t0.10560315171385562\n  :%09:%0A%20%20(0\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:259)\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:217)\n\tat org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:254)\n\tat org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:436)\n\tat org.apache.spark.SparkContext.$anonfun$hadoopFile$2(SparkContext.scala:1126)\n\tat org.apache.spark.SparkContext.$anonfun$hadoopFile$2$adapted(SparkContext.scala:1126)\n\tat org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8(HadoopRDD.scala:181)\n\tat org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8$adapted(HadoopRDD.scala:181)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$6(HadoopRDD.scala:181)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:178)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:201)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.net.URISyntaxException: Relative path in absolute URI:  10896)\t0.10560315171385562\n  :%09:%0A%20%20(0\n\tat java.base/java.net.URI.checkPath(URI.java:1940)\n\tat java.base/java.net.URI.<init>(URI.java:757)\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:256)\n\t... 33 more\n"
          ]
        }
      ]
    }
  ]
}